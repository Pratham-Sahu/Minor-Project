{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image\n",
      "(38398, 30, 30, 3) (38398,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30718, 30, 30, 3) (7680, 30, 30, 3) (30718,) (7680,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt#to plot accuracy\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split #to split training and testing data\n",
    "from keras.utils import to_categorical#to convert the labels present in y_train and t_test into one-hot encoding\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout#to create CNN\n",
    "from keras.preprocessing.image import ImageDataGenerator  # Import the ImageDataGenerator\n",
    "from keras.optimizers import Adam as AdamLegacy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Define the number of images to load from each class\n",
    "num_images_per_class = 2000\n",
    "classes = 43\n",
    "\n",
    "# Define the directory where your dataset is stored\n",
    "dataset_dir = \"/Users/adityachaturvedi/Desktop/7th Sem/Minor Project\"\n",
    "output_dir = \"/Users/adityachaturvedi/Desktop/7th Sem/Minor Project/CleanedData\"\n",
    "\n",
    "# Define the function to clean the data\n",
    "def clean_data(input_dir, output_dir, num_images_per_class):\n",
    "    for i in range(classes):\n",
    "        input_path = os.path.join(input_dir, 'train', str(i))\n",
    "        output_path = os.path.join(output_dir, 'train', str(i))\n",
    "        \n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        \n",
    "        images = os.listdir(input_path)[:num_images_per_class]  # Load only a subset of images\n",
    "        for a in images:\n",
    "            try:\n",
    "                image = Image.open(os.path.join(input_path, a))\n",
    "                image = image.resize((30, 30))\n",
    "                image.save(os.path.join(output_path, a))\n",
    "            except:\n",
    "                print(\"Error cleaning image:\", os.path.join(input_path, a))\n",
    "\n",
    "# Clean the data\n",
    "clean_data(dataset_dir, output_dir, num_images_per_class)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_rate = 0.95\n",
    "    if epoch >= 10:\n",
    "        return initial_learning_rate * decay_rate\n",
    "    return initial_learning_rate\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "classes = 43\n",
    "cur_path = os.getcwd()\n",
    "\n",
    "# Load a smaller subset of images from each class\n",
    "for i in range(classes):\n",
    "    path = os.path.join(cur_path, 'train', str(i))\n",
    "    images = os.listdir(path)[:num_images_per_class]  # Load only a subset of images\n",
    "    for a in images:\n",
    "        try:\n",
    "            image = Image.open(os.path.join(path, a))\n",
    "            image = image.resize((30, 30))\n",
    "            image = np.array(image)\n",
    "            data.append(image)\n",
    "            labels.append(i)\n",
    "        except:\n",
    "            print(\"Error loading image\")\n",
    "\n",
    "\n",
    "#Converting lists into numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(data.shape, labels.shape)\n",
    "\n",
    "data = data / 255.0\n",
    "\n",
    "#Splitting training and testing dataset\n",
    "X_t1, X_t2, y_t1, y_t2 = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(X_t1.shape, X_t2.shape, y_t1.shape, y_t2.shape)\n",
    "\n",
    "# Data augmentation settings\n",
    "#Converting the labels into one hot encoding\n",
    "y_t1 = to_categorical(y_t1, 43)\n",
    "y_t2 = to_categorical(y_t2, 43)\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_t1.shape[1:]))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "# Compile the model with the Adam optimizer\n",
    "optimizer = AdamLegacy(learning_rate=0.001)  # Initial learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training the model with data augmentation\n",
    "eps = 30\n",
    "batch_size = 32\n",
    "\n",
    "# Define the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train the model using augmented data generator with the learning rate scheduler\n",
    "history = model.fit(\n",
    "    datagen.flow(X_t1, y_t1, batch_size=batch_size),\n",
    "    steps_per_epoch=len(X_t1) // batch_size,\n",
    "    epochs=eps,\n",
    "    validation_data=(X_t2, y_t2),\n",
    "    validation_steps=len(X_t2) // batch_size,\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "#plotting graphs for accuracy\n",
    "plt.figure(0)\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#testing accuracy on test dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test = pd.read_csv('Test.csv')\n",
    "print(y_test)\n",
    "labels = y_test[\"ClassId\"].values\n",
    "imgs = y_test[\"Path\"].values\n",
    "data=[]\n",
    "\n",
    "\n",
    "# Load and preprocess test data\n",
    "y_test = pd.read_csv('Test.csv')\n",
    "labels = y_test[\"ClassId\"].values\n",
    "imgs = y_test[\"Path\"].values\n",
    "test_data = []\n",
    "\n",
    "base_dir = \"/Users/adityachaturvedi/Desktop/7th Sem/Minor Project\"\n",
    "\n",
    "for img_path in imgs:\n",
    "    full_img_path = os.path.join(base_dir, img_path)\n",
    "    print(\"Processing image:\", full_img_path)  # Print the full image path\n",
    "    if os.path.exists(full_img_path):\n",
    "        image = Image.open(full_img_path)\n",
    "        image = image.resize((30, 30))\n",
    "        image = np.array(image)\n",
    "        test_data.append(image)\n",
    "    else:\n",
    "        print(\"Image not found:\", full_img_path)  # Print if the image is not found\n",
    "\n",
    "X_test = np.array(test_data)\n",
    "\n",
    "# Predictions on test data\n",
    "pred_probs_test = model.predict(X_test)\n",
    "predicted_classes_test = pred_probs_test.argmax(axis=-1)\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "accuracy_test = accuracy_score(labels, predicted_classes_test)\n",
    "print(\"Test accuracy:\", accuracy_test)\n",
    "\n",
    "#Save the Trained Model\n",
    "model.save('traffic_classifier.keras')\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import tensorflow as tf\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # Define a function to create the model\n",
    "# def create_model(optimizer='adam', dropout_rate=0.25):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=X_t1.shape[1:]))\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
    "#     model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(rate=dropout_rate))\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "#     model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(rate=dropout_rate))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dropout(rate=dropout_rate))\n",
    "#     model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Load and preprocess training data\n",
    "# data = []\n",
    "# labels = []\n",
    "# classes = 43\n",
    "# cur_path = os.getcwd()\n",
    "\n",
    "# for i in range(classes):\n",
    "#     path = os.path.join(cur_path, 'train', str(i))\n",
    "#     images = os.listdir(path)\n",
    "#     for a in images:\n",
    "#         try:\n",
    "#             image = Image.open(os.path.join(path, a))\n",
    "#             image = image.resize((30, 30))\n",
    "#             image = np.array(image)\n",
    "#             data.append(image)\n",
    "#             labels.append(i)\n",
    "#         except:\n",
    "#             print(\"Error loading image\")\n",
    "\n",
    "# data = np.array(data)\n",
    "# labels = np.array(labels)\n",
    "# print(data.shape, labels.shape)\n",
    "\n",
    "# X_t1, X_t2, y_t1, y_t2 = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "# print(X_t1.shape, X_t2.shape, y_t1.shape, y_t2.shape)\n",
    "\n",
    "# y_t1 = to_categorical(y_t1, 43)\n",
    "# y_t2 = to_categorical(y_t2, 43)\n",
    "\n",
    "# param_grid = {\n",
    "#     'optimizer': ['adam', 'rmsprop'],\n",
    "#     'dropout_rate': [0.25, 0.5]\n",
    "# }\n",
    "\n",
    "# y_t1_original = np.argmax(y_t1, axis=1)\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "# results = []\n",
    "\n",
    "# for train_index, val_index in cv.split(X_t1, y_t1_original):\n",
    "#     X_train, X_val = X_t1[train_index], X_t1[val_index]\n",
    "#     y_train, y_val = y_t1_original[train_index], y_t1_original[val_index]\n",
    "    \n",
    "#     y_train_onehot = to_categorical(y_train, num_classes=43)\n",
    "#     y_val_onehot = to_categorical(y_val, num_classes=43)\n",
    "    \n",
    "#     model = create_model(optimizer='adam', dropout_rate=0.25)\n",
    "    \n",
    "#     datagen = ImageDataGenerator(\n",
    "#         rotation_range=10,\n",
    "#         width_shift_range=0.1,\n",
    "#         height_shift_range=0.1,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True,\n",
    "#         fill_mode='nearest')\n",
    "    \n",
    "#     datagen.fit(X_train)\n",
    "    \n",
    "#     model.fit_generator(datagen.flow(X_train, y_train_onehot, batch_size=32),\n",
    "#               steps_per_epoch= len(X_train) // 32,\n",
    "#               epochs=15,\n",
    "#               validation_data=(X_val, y_val_onehot),\n",
    "#               verbose=0)\n",
    "    \n",
    "#     val_score = model.evaluate(X_val, y_val_onehot, verbose=0)\n",
    "#     results.append((val_score[1], 'adam', 0.25))  # Store validation accuracy and hyperparameters\n",
    "\n",
    "# # Print the best hyperparameters and accuracy\n",
    "# best_result = max(results, key=lambda x: x[0])\n",
    "# print(\"Best Accuracy:\", best_result[0])\n",
    "# print(\"Best Optimizer:\", best_result[1])\n",
    "# print(\"Best Dropout Rate:\", best_result[2])\n",
    "\n",
    "# # Building the model with best hyperparameters\n",
    "# best_model = create_model(optimizer=best_result[1], dropout_rate=best_result[2])\n",
    "# best_model.fit_generator(X_t1, y_t1, batch_size=32, epochs=15, validation_data=(X_t2, y_t2), verbose=1)\n",
    "# best_model.save(\"my_model.h5.keras\")\n",
    "\n",
    "\n",
    "# # Train the model and store the training history\n",
    "# history = model.fit(X_train, y_train_onehot, batch_size=32, epochs=15, validation_data=(X_val, y_val_onehot), verbose=1)\n",
    "\n",
    "# # Plotting graphs for accuracy\n",
    "# plt.figure(0)\n",
    "# plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "# plt.title('Accuracy')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.plot(history.history['loss'], label='training loss')\n",
    "# plt.plot(history.history['val_loss'], label='val loss')\n",
    "# plt.title('Loss')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.ylabel('loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Load and preprocess test data\n",
    "# y_test = pd.read_csv('Test.csv')\n",
    "# labels = y_test[\"ClassId\"].values\n",
    "# imgs = y_test[\"Path\"].values\n",
    "# test_data = []\n",
    "\n",
    "\n",
    "# base_dir = \"/Users/adityachaturvedi/Desktop/7th Sem/Minor Project\"\n",
    "\n",
    "\n",
    "# for img_path in imgs:\n",
    "#     full_img_path = os.path.join(base_dir, img_path)\n",
    "#     print(\"Processing image:\", full_img_path)\n",
    "\n",
    "#     if os.path.exists(full_img_path):\n",
    "#         image = Image.open(full_img_path)\n",
    "#         image = image.resize((30, 30))\n",
    "#         image = np.array(image)\n",
    "#         test_data.append(image)\n",
    "#     else:\n",
    "#         print(\"Image not found:\", full_img_path)\n",
    "\n",
    "# X_test = np.array(test_data)\n",
    "\n",
    "# # Predictions on test data\n",
    "# pred_probs_test = best_model.predict(X_test)\n",
    "# predicted_classes_test = pred_probs_test.argmax(axis=-1)\n",
    "\n",
    "# # Calculate accuracy on test data\n",
    "# accuracy_test = accuracy_score(labels, predicted_classes_test)\n",
    "# print(\"Test accuracy:\", accuracy_test)\n",
    "\n",
    "# # Save the Trained Model\n",
    "# best_model.save('traffic_classifier.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
